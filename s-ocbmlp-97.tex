\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
\usepackage[right=0.8in, top=1in, bottom=1in, left=0.8in]{geometry}
\usepackage{setspace}
\usepackage{bbold}
\usepackage[backend=bibtex,style=ieee]{biblatex}
\usepackage{stmaryrd}
\addbibresource{biblio}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\newcommand{\E}{\textbf{E}}
\newcommand{\var}{\text{var}}

\title{Computational Learning Theory - Project Report}
\author{Patanjali Vakhulabharanam (pv2270)\\ Shivani Gupta (sg3296)}

\begin{document}

\maketitle
\tableofcontents
\newpage
\section{Introduction}
    We present a review of the paper \cite{Guruswami:1999:MLB:307400.307429} titled "Multiclass Learning, Boosting, and Error-Correcting Codes" by Venkatesan Guruswami and Amit Sahai. The paper broadly does the following:
    \begin{itemize}
    \item Proves bounds on performance of multiclass learning using Error Correcting Codes
    \item Compares the performance of multiclass learning to one-per-class approach and identifies 'Error-Correlation' as a parameter to choose the appropriate method.
    \item Proposes a new variant of multiclass \textsc{AdaBoost} i.e. \textsc{AdaBoost}.ECC and gives an analysis of its performance
    \end{itemize}
    In this review we tried to present as much background as possible on terminology and relevant results, especially on things we found confusing or difficult to follow in our first reading of the paper. As such, some sections might seem a bit lengthy or extraneous\\\\
	Multiclass learning is the task of classifying examples into k classes where $k > 2$. Most well established algorithms are best suited for learning binary classification problems. This paper presents some results on how to combine  multiple binary class learners effectively to produce an efficient multiclass learner.
	
\section{Concepts Used}
    \subsection{Error Correcting Codes}
        \bf Definition: \normalfont \textit {An (n,K,d) error-correcting code C is a set of K binary vectors of dimension n, called codewords, such that the Hamming distance between every pair of distinct codewords is at least d}.\\
        \bf Property: Code $C$ can correct at least $\lfloor \frac{d-1}{2} \rfloor$ errors. \normalfont\\
        Since the distance between any two codewords is at least d, to make a given codeword closer to another than itself, we should invert at least half the difference, which is $\lfloor \frac{d-1}{2} \rfloor$. In other words, any codeword can absorb at most  $\lfloor \frac{d-1}{2} \rfloor$ errors.\\

    \subsection{Hadamard Matrix Codes}
        \bf Definition: \normalfont \textit{$\left\{ H_{n+1} \right\}$ of $(2^{n+1},2^{n+1},2^n)$ is a $2^{n+1}$ by $2^{n+1}$ matrix where all the codewords(as defined by each row or the columns) are at least $2^n$ Hamming distance apart.}\normalfont\\
        The construction of Hadamard Matrix Codes is done recursively as follows:
        
        \[
                H_{n+1} = \left[{ \begin{array}{cc}
                         H_n & H_n\\
                         H_n & -H_n      \end{array} } \right]
            \]\\
            \begin{claim}
                The iterative approach maintains the minimum Hamming distance of $2^n$ between any two codewords x and y.
            \end{claim}
            \begin{proof}
                The property can be proved inductively:
                \begin{enumerate}
                \item The code for $H_1$ i.e. (2,2,1) code is given as follows:
                
                    \[
                        H_1=
                            \left[ {\begin{array}{cc}
                            1 & 1 \\       1 & -1       \end{array} } \right]
                    \]\\
                    It can be easily seen that the two codes thus produced are at a distance of 1 and satisfy the property
                \item Assuming $H_n$ has all codewords at least $2^{n-1}$ distance apart we need to prove that $H_{n+1}$ has any two codewords x and y which are at least at a distance of $2^n$. Two conditions follow:
                    \begin{enumerate}
                        \item If x and y have first $2^n$ bits same then the last $2^n$ bits will definitely be different as ensured by the construction. Thus the codewords are at a distance of $2^n$.
                        \item If the first $2^n$ bits are not same then they will be at least different at $2^{n-1}$ positions. Also the last $2^n$ positions which are same for x and negated for y, the hamming distance would again be at least $2^{n-1}$. Therefore any two such codewords also preserve the property. 
                    \end{enumerate}
            \end{enumerate}
            \end{proof}
    \subsection{Colorings}
        A coloring $\mu : Y \rightarrow \{-1, +1\}$ is a function mapping a set of binary vectors Y onto the set $\{-1,+1\}$. This could for instance be one bit of the binary encoding of Y
    \subsection{Closest codeword decoding}
    Given a set of codes C and a string $x$ of length $n$, the closest codeword decoding of $x$ gives the code from C that has the smallest hamming distance to $x$.
    \subsection{Weighted closest codeword decoding}
    Given a set of codes C of length n, and two strings $\alpha \in \mathbb{R}^n$ and $\beta \in \mathbb{R}^n$ which represent the weights of each bit being a $+1$ or a $-1$ of a string x respectively, the closest codeword decoding of x gives the code c from C that has the smallest weighted sum $\sum_{i=1}^n (\alpha_i\llbracket c_i = +1 \rrbracket + \beta_i\llbracket c_i = -1 \rrbracket)$
    \subsection{Pseudoloss}
    Given the following
    \begin{itemize}
    \item Multiclass classification problem with m examples $x_i \in X$ and labels $y_i \in Y$ where $|Y| = k$
    \item Coloring $\mu : Y \rightarrow \{-1,+1\}$
    \item Hypothesis $h: X \rightarrow \{-1,+1\}$ to predict the coloring $\mu$
    \item Probability distribution on labellings $D : X \times Y \rightarrow [0,1]$ with the constraint that correctly labelled pairs $(x_i, y_i)$ have 0 probability
    \end{itemize}
    The pseudoloss of the hypothesis $h$ for the coloring $\mu$ on the distribution D is calculated as sum of two things
    \begin{itemize}
    \item If $y_i \in \{y : h(x_i) == \mu(y) \}$, 0, otherwise half the sum of all entries in D for $x_i$
    \item Half the sum of all entries of $D(x_i, y)$ where $y \in \{y : h(x_i) == \mu(y) \}$
    \end{itemize}
    It is essentially a measure of the weight of the distribution on
    all the misclassifications of h (note that since the probabilities of correct labellings are zero, although we add them in the above expressions, they contribute 0)
\section{Approaches to Multiclass Learning}
	At their core most multiclass learning algorithms reduce multiclass learning to binary classification problems.
	
	Binary encoding:
	\begin{itemize}
		\item If we have k output classes, encode each of the k classes into a unique binary string of length n.
		\item The shortest n possible is trivially $\lceil \log k \rceil$ and codes longer than k usually wouldn't make sense
	\end{itemize}
    \subsection{One per class approach(one vs all)}
        The one per class approach encodes into a binary string of length $n=k$ where the encoding of the $i^{th}$ class has the $i^{th}$ bit as +1 and all others -1. Then n binary hypothesis are learnt corresponding to each bit of the code. For a given new instance, all n hypotheses are evaluated and the one that output +1 is the output class. Ties are broken arbitrarily
    \subsection{ECC encoding}
        Encode each of the k classes into binary strings of length n in such a way that any pair of encodings has at least a minimum hamming distance greater than a certain threshold. One per class can be considered ECC with distance exactly 2

\section{Using Error Correcting Codes}
    The algorithm for learning multiclass from Error Correcting codes can be given as follows:
    \begin{enumerate}
        \item For k classes to be learned choose a (n,k,d) code C. C would be a k x n matrix where a row i is a n bit string corresponding to code for $i^{th}$ class.
        \item Hypothesis $h_1,h_2....h_n$ need to be learned. For hypothesis $h_j$ to be learned use a binary learner and example set labelled corresponding to the $j^{th}$ bit. Thus we learn all n hypothesis.
        \item Predict the label of a new instance by running the n hypothesis, to produce a n bit string S. Then find the n-bit string($i^{th}$) row in the matrix that has the least hamming distance and label the example as belonging to class i.
    \end{enumerate}
    \begin{claim}
        Even if $\lfloor\frac{d-1}{2}\rfloor$ of the hypothesis predicted incorrectly, the example is labelled with the correct class
    \end{claim}
   \begin{proof}
        This follows immediately from the property of error correcting codes. The codes are at least a distance d apart. Therefore, even if the mistaken bits are less then $\lfloor\frac{d-1}{2}\rfloor$ it will be still closer to the correct label's n bit code rather then any other code
    \end{proof}
    
\section{Comparison Between ECC approach and One Per Class Approach}
The first two lemmas of this section show that the worst case training erorr using ECC is much lower than the worst case error using one per class approach. The next lemma shows that the worst case error of ECC can be twice as high as the best case error of one per class meaning that ECC does not always necessarily outperform one per class. Also, the nature of the hypotheses in ECC is complex and learning them might be inherently more difficult. 

The last lemma of this section proves bounds on the worst case error rate in terms of the bounds of pair-wise error correlations between the hypotheses. What this means is that if we are able to somehow bound the correlations of the errors, we can get good training performance using ECC

   \begin{lemma} 
        The worst-case training error of one-per-class can be as high as $min\left\{\sum_{i=1}^k e_i , 1\right\}$; and for randomized one-per-class it can be as high as $min\left\{\frac{k-1}{k}\sum_{i=1}^k e_i,1\right\}$
   \end{lemma}
   \begin{proof} 
        Two parts of the proof follow:
         \begin{enumerate}
                \item One-Per Class\\
                    There are k classes where each class is represented by a binary hypotheses $h_1,h_2,...h_k$. Also suppose that each hypotheses $h_i$ makes a fractional training error $e_i$ i.e. when presented with 
                    \begin{enumerate}
                        \item a sample that belongs to the class i and $h_i$ it labels it as negative
                         \item a sample that does not belong to the class i and $h_i$ labels it as positive
                    \end{enumerate}
                    with at most error $e_i$. Therefore, the worst case happens when the error made by each hypothesis is only on the false negatives. In this case a hypothesis $h_i$ mislabels a positive example as negative and no other hypothesis labels it positive. Thus the examples on which all the hypothesis make error are disjoint. Hence the worst case error is their sum i.e. $min\left\{\sum_{i=1}^k e_i , 1\right\}$ 
        
                \item Randomized One-Per-Class\\
                    In the randomized one-per-class approach following the worst case approach as in one-per-class such that each hypothesis $h_i$ makes error $e_i$ only on false negatives, then for the given samples no hypothesis labels it positive. In this case the randomized one-per-class approach chooses randomly a class from the k classes. However, only one label is correct, leaving $\frac{k-1}{k}$ fraction of choices wrong. Therefore, the worst case error for the randomized approach is bounded by $min\left\{\frac{k-1}{k}\sum_{i=1}^k e_i,1\right\}$.
         \end{enumerate} 
    \end{proof}
    
    \begin{lemma}
        The worst case training error of ECC approach using an (n,k,d) code can be no higher than 2n/d times the average error $\frac{1}{n}\sum_{i=1}^n e_i$ of the binary hypotheses.
    \end{lemma}
    \begin{proof}
        If there are N training examples and each is mislabelled by each of the n hypothesis then the total error would be $N \sum_{i=1}^n e_i$.\\
        However the final outcome is dependent on the hamming distance with the n bit strings representing the classes which are at least d distance apart. Therefore only when more then $\lfloor\frac{d-1}{2}\rfloor$ hypothesis mislabel a given example, the final class labelling is wrong.\\
        Thus each example must have at least $d/2$ hypothesis out of the n predicting wrongly for it to be mislabelled. Therefore the errors are bounded by:
    
        \begin{align*}
            Errors &\leq \frac{N}{d/2} \sum_{i=1}^n e_i\\
            &\leq N \left(\frac{2n}{d} \frac{\sum_{i=1}^n e_i}{n}\right)
         \end{align*}
        Hence training error of ECC approach using an (n,k,d) code can be no higher than 2n/d times the average error $\frac{1}{n}\sum_{i=1}^n e_i$ of the binary hypotheses.
    \end{proof}
    
    
            \begin{claim}
                For any codeword x, inverting the last n/2 positions transforms it into another codeword y. Hence, inverting the last n/4+1 positions of x creates a vector closer to y then to x.
            \end{claim}
            \begin{proof}
                As stated before if the distance between two codewords x and y is d (here n/2), then the vector formed by inverting at most $\lfloor\frac{d-1}{2}\rfloor$ bits of x is still closer to x. But inverting the next bit makes it closer to y i.e. $\lfloor\frac{d-1}{2}+1\rfloor$  =  = $\lfloor\frac{n}{4}+1/2\rfloor$ = n/4+1. For example:
                \begin{align*}
                    x & = 111111\\
                    y & = 111000\\
                    z & = 111011
                \end{align*}
                Here y has been constructed by flipping last n/2 bits i.e. 3 bits and z by flipping n/4+1 i.e. last two bits of x. As can be seen codeword z is closer to y then to x.
            \end{proof}
                
            \begin{claim} 
                    For any codeword x, there is some subset (which depends on x) of its first n/2 + 1 positions that, when inverted, creates a vector that is closer to some codeword different from x than it is to x
            \end{claim}
            \begin{proof}
                Let us define the following
                \begin{itemize}
                    \item Codeword y: (n/2+1)$^{th}$ bit and some subset of first n/2 positions inverted.
                    \item Vector v: same as y on first (n/2+1) positions and equal to x on last (n/2-1) positions
                \end{itemize}
                 Since y is a codeword from the Hadamard Matrix and has been constructed iteratively as explained before, the difference between the first n/2 positions of x and y would be exactly n/4. Also, since x and y differ on their (n/2+1)th position therefore, they must differ exactly n/4 times in the last n/2 positions as well.//
                 Vector v is then different from x at exactly (n/4)+1 positions but (n/4)-1 positions from y, therefore vector v is closer to y. For example:
                 \begin{align*}
                      x & =11111111\\
                      y & =00110011\\
                      v & =00110111 
                 \end{align*}
                 As can be seen x,y and z are constructed as explained. Further v is at a distance 3 from x whereas at a distance 1 from y. Thus v is closer to y.\\\\
            \end{proof}
    \begin{lemma} 
        The worst case training error of an ECC approach using an (n,k,n/2) code, where n is a power of 2, can be as high as 2 times the best error of binary hypotheses.
    \end{lemma}
    \begin{proof}
    The construction of the worst case situation such that it matches the Claims proved above. Therefore if we have n binary hypotheses such that $h_i$ makes error $e_i$, we consider the following two cases:
    \begin{enumerate}
        \item Suppose the n/4+1 hypothesis : $h_{3n/4},....h_{n}$ all make mistake on a fraction $f_1$ of training examples. From Claim 5, all the examples in $f_1$ would be mislabelled (as nearer to codeword y then to x).
        Since, the last n/4+1 hypothesis are making error, in the worst case where the errors are made on the same examples, $f_1$ would be $min\left\{e_{3n/4},...e_n\right\}$
        \item Now suppose there is a fraction $f_2$ of examples such that if we invert some subset of n/2+1 positions in it, then by Claim 6 it will be closer to another codeword y and would be misclassified. Here, the first n/2+1 hypothesis are making error, in the worst case where the errors are made on the same examples, $f_2$ would be $min\left\{e_{1},...e_{n/2+1}\right\}$.
    \end{enumerate}
    Thus total error=
    \begin{align*}
        f_1+f_2 &= min\left\{e_{3n/4},...e_n\right\}+min\left\{e_{1},...e_{n/2+1}\right\}\\
            &\geq 2  min\left\{e_1,...e_n\right\}
    \end{align*}
    \end{proof}
    The ECC approach does not suffice when the errors are highly correlated, here one-per-class works better. Therefore next result works on providing an upper bound on error taking error correlation of two hypothesis into consideration
    \begin{lemma} 
        Let $\Delta$ be an upper bound on $Pr_x[h_i(x)$ is wrong and $h_j(x)$ is wrong] for all i,j such that $1 \leq i <j \leq n$. Then the worst-case training error of ECC approach using an (n,k,d) code can be no higher then $4\frac{n(n-1)}{d(d-2)}\Delta$. In particular, for codes with minimum distance d at least n/2+2, this is no more than 16$\Delta$\\
    \end{lemma} 
     \begin{proof}
        Suppose that the total no of training errors performed on a training sample of N examples is given by a fraction $\epsilon$. Then the total no of mistakes would be $\epsilon N$. Each of these mistakes occur only when at least d/2 of the n hypothesis predicted incorrectly. Therefore maximum number of combinations such that $h_i$ and $h_j$ were wrong on the same example is given by $d/2 \choose 2$. Hence for all the $\epsilon N$ mistakes this evaluates to $\epsilon N {d/2 \choose 2}$.\\
        Also,\\
        Pr[$h_i(x)$ is wrong and $h_j(x)$ is wrong on 1 example] = $\Delta$\\
        Pr[$\forall_{1\leq i < j \leq n} h_i(x)$ and $h_j(x)$ wrong on 1 example]=${n \choose 2} \Delta$\\
        Pr[$\forall_{1\leq i < j \leq n} h_i(x)$ and $h_j(x)$ wrong on N examples]=${n \choose 2} \Delta N$\\\\
        This thus gives an upper bound on the number of errors and we have the following relation:
        \begin{align*}
            \epsilon N {d/2 \choose 2} &\leq {n \choose 2} \Delta N\\
            \epsilon &\leq 4\frac{n(n-1)}{d(d-2)}\Delta
        \end{align*}
    \end{proof}
    From the above relation it is evident that the error bound for ECC approach is directly proportional to the error correlation between the hypothesis. If the correlation is high so is the upper bound and ECC approach may not be a good approach in this case.
\normalfont
    
\section{Boosting}
We maintain a notation where a bar on top of variables mean that these represent some variables in an equivalent problem of \textsc{AdaBoost}

\subsection{\textsc{AdaBoost}.OC}
Schapire\cite{s-ocbmlp-97} introduces \textsc{AdaBoost}.OC that builds upon \textsc{AdaBoost} boosting algorithm to solve multiclass problems. The \textsc{AdaBoost}.ECC as defined by this paper improves upon \textsc{AdaBoost}.OC. The description of the algorithm is as follows:
\begin{itemize}
\item Input sequence of labelled examples $(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)$ where the inputs $x_i \in X$ and $y_i \in Y$, $|Y| = k$
\item Initialize distribution $\bar D_1$
\begin{itemize}
\item $\bar D_1(i, l) = \frac{1}{m(k-1)}\quad \forall i \in [m],\quad l \in Y - \{y_i\}$
\item $\bar D_1(i, l) = 0 \quad \forall i \in [m],\quad l = y_i$
\end{itemize}
\item For $t =  1 \ldots T$
\begin{itemize}
\item Compute a coloring $\mu_t : Y \rightarrow \{0,1\}$
\item Calculate $U_t = \sum_{i=1}^m \sum_{l \in Y} \bar D_t(i,l) \llbracket \mu_t(y_i) \neq \mu_t(l)\rrbracket$
\item Calculate $D_t(i) = \frac{1}{U_t} \sum_{l \in Y} \bar D_t(i,l) \llbracket \mu_t(y_i) \neq \mu_t(l)\rrbracket$
\item Use weak learner to learn hypothesis $h_t : X \rightarrow \{-1,+1\}$ on the m examples labelled according to coloring $\mu_t$ and distribution $D_t$
\item Calculate $\bar h_t(x) = \{l \in Y : h_t(x) = \mu_t(l)\}$
\item Calculate pseudoloss $\bar \epsilon_t$ of hypothesis $h_t$ on distribution $D_t$\\
$\bar \epsilon_t = \frac{1}{2} \sum_{i=1}^m \sum_{l \in Y} (\llbracket y_i \notin \bar h_t(x_i)\rrbracket + \llbracket l \in \bar h_t(x_i) \rrbracket)$
\item Calculate weight $\alpha_t = \frac{1}{2} \ln \left( \frac{1-\epsilon_t}{\epsilon_t}\right)$
\item Update $\bar D_{t+1}(i,l) = \frac{1}{\bar Z_t} \bar D_t(i,l) exp \left\{ \alpha_t(\llbracket y_i \notin \bar h_t(x_i)\rrbracket + \llbracket l \in \bar h_t(x_i) \rrbracket)\right\}$. Here $\bar Z_t$ is a normalization factor.
\end{itemize}
\item Output final hypothesis $$H(x) = argmax_{l \in Y} \sum_{t=1}^T \alpha_t \llbracket l \in \bar h_t(x) \rrbracket$$
\end{itemize}

\subsubsection{Coloring Function $\mu_t$}
\textsc{AdaBoost}.OC for multiclass problem basically maps the multiclass problem to binary by using the coloring function $\mu_t$ at each iteration t.  The coloring $\mu_t: Y \to \left\{-1,+1\right\}$ is thus important for good performance of the algorithm. It essentially partitions the k labels into two subsets and labels them as -1 and 1. The same coloring method has also been used in \textsc{AdaBoost}.ECC as presented in this paper. Here we present a very brief approach to evaluate $\mu_t$. As proved by Schapire in Theorem 1 of \cite{s-ocbmlp-97} the training error of final hypothesis is bounded by:
\begin{align*}
    (k-1) \prod_{t=1}^T \sqrt{1-4(\gamma_t U_t)^2} \leq (k-1)exp\left(-2 \sum_{t=1}^T (\gamma_t U_t)^2\right)
\end{align*}
Therefore, the coloring function should be such that it maximizes $U_t$.
\begin{align*}
    U_t = \sum_{i=1}^m \sum_{l \in Y} \bar D_t(i,l) \llbracket \mu_t(y_i) \neq \mu_t(l)\rrbracket
\end{align*}
There are different methods that can be used to generate the coloring function $\mu_t$ at each iteration. The value can be maximized if the probability of $\mu_t(y_i) \neq \mu_t(l)$ = 1/2 for each label l. To achieve this the value of $\mu_t(l)$ is chosen randomly from $\{-1,+1\}$. Thus the expected value of $U_t$ over all examples and all labels also becomes 1/2. At each level of boosting we thus achieve a distribution with equal positive and negative labels from the last coloring function.

\section{$\textsc{AdaBoost}$.ECC}
The main algorithm, termed \textsc{AdaBoost}.ECC and defined below, somewhat follows the structure of \textsc{AdaBoost}.OC described in the previous section and improves on its performance. The improvements are two fold.
Prediction efficiency wise, the following theorems and lemmas show that the error rate of \textsc{AdaBoost}.ECC is at least as good as \textsc{AdaBoost}.OC.
The algorithm does not explicitly calculate pseudoloss and works directly with the errors of the binary classifiers and therefore runs much faster in practice(less running time)

A high level description of the algorithm is provided below
\begin{itemize}
\item Initializes a distribution that is uniform on all combinations of input examples and wrong labellings $\bar D$ and 0 on combinations of input examples and correct labellings
\item Iterates T times and in iteration
\begin{itemize}
\item Splits labels into two sets(coloring) and sets up a binary classification problem to predict which set an example belongs to
\item Creates a distribution $D$ corresponding to this binary classification problem by adding relevant entries from $\bar D$ for each example and normalizing
\item Builds a weak learner for this binary classification problem
\item Calculates the weights of positive and negative votes. This can be done in different ways - symmetric, with equal weight to both or asymmetric with different weights(more efficient than symmetric)
\item Update the $m \times k$ distribution $\bar D$ by up weighting misclassifications and down weighting correct classifications
\end{itemize}
\item Gives an output hypothesis that is : label with the highest sum of weights from the individual hypotheses of the T iterations
\end{itemize}

A full detailed description of the algorithm is provided below

\begin{itemize}
\item Input sequence of labelled examples $(x_1, y_1), (x_2, y_2), \ldots, (x_m, y_m)$ where the inputs $x_i \in X$ and $y_i \in Y$, $|Y| = k$
\item Initialize distribution $\bar D_1$
\begin{itemize}
\item $D_1(i, l) = \frac{1}{m(k-1)}\quad \forall i \in [m],\quad l \in Y - \{y_i\}$
\item $D_1(i, l) = 0 \quad \forall i \in [m],\quad l = y_i$
\end{itemize}
\item For $t =  1 \ldots T$
\begin{itemize}
\item Compute a coloring $\mu_t : Y \rightarrow \{0,1\}$
\item Calculate $U_t = \sum_{i=1}^m \sum_{l \in Y} \bar D_t(i,l) \llbracket \mu_t(y_i) \neq \mu_t(l)\rrbracket$
\item Calculate $D_t(i) = \frac{1}{U_t} \sum_{l \in Y} \bar D_t(i,l) \llbracket \mu_t(y_i) \neq \mu_t(l)\rrbracket$
\item Use weak learner to learn hypothesis $h_t : X \rightarrow \{-1,+1\}$ on the m examples labelled according to coloring $\mu_t$ and distribution $D_t$
\item Calculate weights of positive and negative votes
\item For asymmetric version
\begin{itemize}
\item $\alpha_t = \frac{1}{2} \ln \left(\frac{\sum_{i:h_t(x_1) = \mu_t(y_i) = 1} D_t(i)}{\sum_{i:h_t(x_1) = 1 \neq \mu_t(y_i) } D_t(i)} \right)$
\item $\beta_t = \frac{1}{2} \ln \left(\frac{\sum_{i:h_t(x_1) = \mu_t(y_i) = -1} D_t(i)}{\sum_{i:h_t(x_1) = -1 \neq \mu_t(y_i) } D_t(i)} \right)$
\end{itemize}
\item For symmetric version
\begin{itemize}
\item $\alpha_t = \beta_t = \frac{1}{2} \ln \left(\frac{\sum_{i:h_t(x_1) = \mu_t(y_i)} D_t(i)}{\sum_{i:h_t(x_1) \neq \mu_t(y_i) } D_t(i)} \right)$
\end{itemize}
\item Define $g_t(x) = $
\begin{itemize}
\item $\alpha_t$ if $h_t(x) = +1$
\item $-\beta_t$ if $h_t(x) = -1$
\end{itemize}
\item Update $\bar D_{t+1}(i,l) = \frac{1}{\bar Z_t} \bar D_t(i,l) exp(\frac{g_t(x_i)}{2}(\mu_t(l)-\mu_t(y_i)))$, where $\bar Z_t$ is a normalization factor
\end{itemize}
\item Output final hypothesis $$H(x) = argmax_{l \in Y} \sum_{t=1}^T g_t(x)\mu_t(l)$$
\end{itemize}

Before we prove the theoretical guarantees of this algorithm present in the paper, we introduce a few other results that will be used in their proofs. 
The following theorem on a generalized analysis of \textsc{AdaBoost} with unspecified weights $\alpha_t$ from  \cite{Schapire:1999:IBA:337859.337870} gives bounds on errors
\begin{theorem}
If we run \textsc{AdaBoost} with arbitrary $\alpha_t$ (instead of $\alpha_t = \frac{1}{2} \ln \frac{1-\epsilon_t}{\epsilon_t}$), the following bounds on the error of the final hypothesis still hold
$$\frac{1}{m}|\{i : H(x_i) \neq y_i\}| \le \prod_{t=1}^T Z_t$$
\end{theorem}
\begin{proof}
Note that the $Z_t$ themselves are a function of $\alpha_t$ and therefore the bounds are not independent of the choice of $\alpha_t$. The only claim is that the structural form of the bound in terms of $Z_t$ does not change.
The proof follows along similar lines as that of the original \textsc{AdaBoost} algorithm. Using the same notation as used in class, if we unravel the update rule for D and write $D_{T+1}$, we get
\begin{align*}
D_{T+1}(i) &= \frac{exp(-\sum_t \alpha_t y_i h_t(x_i))}{m\prod_t Z_t}\\
&= \frac{exp(-y_i f(x_i))}{m \prod_t Z_t}
\end{align*}
For each i, we have
\begin{itemize}
\item If $H(x_i) \neq y_i$, by definition, $f(x_i) y_i \leq 0$ and therefore $exp(-f(x_i) y_i) \ge 1 = \llbracket H(x_i) \neq y_i \rrbracket$
\item If $H(x_i) = y_i$, since the $exp$ function is non negative, $exp(-f(x_i) y_i) \ge 0 = \llbracket H(x_i) \neq y_i \rrbracket$
\end{itemize}
Using this, we have
\begin{align*}
\sum_{i=1}^m D_{T+1}(i) &\ge \frac{\sum_{i=1}^m \llbracket H(x_i) \neq y_i \rrbracket}{m\prod_t Z_t}\\ 
\end{align*}
Since $D_{T+1}$ represents a probability distribution, we have the LHS above summing to 1 and therefore,
$$\frac{1}{m}|\{i : H(x_i) \neq y_i\}| \le \prod_{t=1}^T Z_t$$
\end{proof}

\begin{lemma} \label{lemma10}
For some real numbers $0 \le U_t \le 1$ and $0 \le \gamma_t \le 1/2$ we have $$U_t \sqrt{1-4\gamma_t^2} + 1-U_t \le \sqrt{1-4\gamma_t^2U_t^2}$$
\end{lemma}
\begin{proof}
If we set $a = U_t \sqrt{1-4\gamma_t^2} + 1-U_t$ and $b = \sqrt{1-4\gamma_t^2U_t^2}$, a and b are clearly non-negative. 
\begin{align*}
b^2-a^2 &= 2U_t(1-U_t)(1-\sqrt{1-4\gamma_t^2})\\
&\ge 0
\end{align*}
since each term in the product is $\ge 0$
\end{proof}
Theoretical guarantees on the performance of this algorithm follow from the following two theorems
\begin{theorem}
The training error of the hypothesis $H$ under the uniform distribution on training examples is at most $$(k-1)\prod_{i=1}^T \bar Z_t = (k-1)\prod_{i=1}^T (Z_t U_t + 1-U_t)$$ where $$Z_t = \sum_{i=1}^m D_t(i) exp(-g_t(x_i)\mu_t(y_i))$$
\end{theorem}
\begin{proof}
The proof proceeds by 
\begin{itemize}
\item Showing an equivalence between each iteration of \textsc{AdaBoost}.ECC and an iteration of the original \textsc{AdaBoost}
\item Using results for \textsc{AdaBoost}
\end{itemize}
Consider a binary learning problem $X \times Y \rightarrow \{-1,+1\}$ with
\begin{itemize}
\item All labels $-1$
\item Uniform distribution on the $k-1$ wrong labels for each example $x_i$(and 0 on corrct labelling). Each example gets a weight $\frac{1}{m(k-1)}$. This is exactly the same as the distribution $\bar D_1$ that we defined in \textsc{AdaBoost}.ECC
\end{itemize}

Under this setting, the update rule for $\bar D_t(i,l)$ of the original \textsc{AdaBoost} is exactly the same as the update rule we defined for \textsc{AdaBoost}.ECC on the original problem. By definition, we have the output hypothesis of the \textsc{AdaBoost} learner to
$$\bar f_t(x_i,l) = \frac{g_t(x_i)}{2}(\mu_t(l) - \mu_t(y_i))$$
If we choose all the weights $\bar \alpha_t$ of the \textsc{AdaBoost} algorithm to be 1, its final hypothesis will be 
$$\bar H(x_i,l) = sign(\sum_{t=1}^T f_t(x_i, l)) $$
If $H$ makes a mistake on example $x_i$, then by definition, we should have 
$$\sum_{t=1}^T g_t(x_i)(\mu_t(H(x_i) - \mu_t(y_i)) \ge 0$$
Using the result of previous theorem and combining the above three equations, for any error of H, ($H(x_i) \neq y_i)$) we should have $$\hat H(x_i, H(x_i)) = 1$$
\begin{align*}
Pr_{x_i \sim U}[H(x_i) \neq y_i] &\le  Pr_{x_i \sim U}[\exists l \neq y_i : \bar H(x_i, l) = 1]\\
&\le (k-1) Pr_{(x_i, l) \sim \bar D_1} [\bar H(x_i, l) = 1]\\
&\le (k-1) \prod_{t=1}^T \bar Z_t
\end{align*}
Now, converting $\bar Z_t$ to $Z_t$
\begin{align*}
\bar Z_t &= \sum_i \sum_l \bar D_t(i,l) exp\left(\frac{g_t(x_i)}{2}(\mu_t(l)-\mu_t(y_i)\right)\\
&= \sum_i exp(-g_t(x_i)\mu_t(y_i)) \sum_{l: \mu_t(l) \neq \mu_t(y_i)} \bar D_t(i,l) + \sum_i \sum_{l: \mu_t(l) = \mu_t(y_i)} \bar D_t(i,l)\\
&= U_t \sum_i D_t(i) exp(-g_t(x_i) \mu_t(y_i)) + (1-U_t)
\end{align*}
This completes the proof
\end{proof}
\begin{theorem}
Let $\mu_1, \mu_2, \ldots \mu_T$ be any sequence of colorings and let $h_1, h_2, \ldots h_T$ be any sequence of weak hypotheses produced by the weak learner. Let $\epsilon_t = 1/2 - \gamma_t$ be the error of $h_t$ with respect to the relabeled and re weighted data on which it was trained and let $U_t$ be as defined in the algorithm. Then the training error of the final hypothesis $H$ output by the symmetric version of \textsc{AdaBoost}.ECC is bounded by 
$$(k-1)\prod_{t=1}^T \left(U_t \sqrt{1-4\gamma_t^2} + 1-U_t\right)$$
\end{theorem}
\begin{proof}
If we use symmetric weights, the proof follows along similar lines as the proof of the corresponding section of \textsc{AdaBoost}. 
We have $\alpha_t = \beta_t = \frac{1}{2} \ln (\frac{1-\epsilon_t}{\epsilon_t})$ and for this choice, we get 
\begin{align*}
Z_t &= \sum_{i=1}^m D_t(i) exp(-\alpha_t h_t(x_i) \mu_t(y_i))\\
&= e^{\alpha_t}\epsilon_t + e^{-\alpha_t}(1-\epsilon_t)\\
&= 2 \sqrt(\epsilon_t(1-\epsilon_t))\\
&= \sqrt{1-4\gamma_t^2}
\end{align*}
Replacing the values of $Z_t$ in the result of the previous theorem, we get
$$Error \le (k-1)\prod_{t=1}^T \left(U_t \sqrt{1-4\gamma_t^2} + 1-U_t\right)$$
\end{proof}
Using the result of Thorem \ref{lemma10}, on the above equation, we get
$$Error \le (k-1)\prod_{t=1}^T \sqrt{1-4\gamma_t^2 U_t^2} $$ which is the error bound of \textsc{AdaBoost}.OC. Therefore, the symmetic version \textsc{AdaBoost}.ECC is at least as good as \textsc{AdaBoost}.OC

\printbibliography[heading=bibintoc,title={References}]

\end{document}